---
title: "431 Project Study 1 Demonstration"
author: "Thomas E. Love"
date: "`r Sys.Date()`"
linkcolor: blue
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    code_folding: show
---

# Introduction 

This document demonstrates the sorts of analyses we are asking you to complete in the context of Task E of your project for Study 1 (using the class survey.) We will use data from a prior class survey, gathered in two data files (called `survey2015raw_a` and `survey2015raw_b`) available on [the Data and Code page of our website](https://github.com/THOMASELOVE/431-2018-data).

# R Preliminaries and Data Load/Merge

## Initial Setup and Package Loads in R 

```{r knitr_init, cache=FALSE, message = FALSE, warning = FALSE}
#library(pander); library(mice); library(Epi)
#library(gridExtra); library(vcd); library(Hmisc)
#library(mosaic); library(magrittr) 

library(knitr); library(rmdformats); library(skimr) 
library(tidyverse) 

source("Love-boost.R")

## Global options

options(max.print="75")
opts_chunk$set(comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)

## Skim options (leave out histograms)

skimr::skim_with(numeric = list(hist = NULL),
                 integer = list(hist = NULL))
```

## Loading the Raw Data into R 

```{r data_load, message = FALSE}
sur15_a <- read.csv("survey2015raw_a.csv") %>% tbl_df
sur15_b <- read.csv("survey2015raw_b.csv") %>% tbl_df
```

## Merging the Two Raw Data Sets 

The `survey2015raw_a.csv` file contains, among other things, the `sex`, `english`, `prior.r`, `height`, `weight`, `comfort.431`, and `load.431` variables. The other variables we'll use are in the `survey2015raw_a.csv` file.

We'll begin by merging the `sur15_a` and `sur15_b` tibbles (each of which contains the linking variable `S.id`, into a data frame called `sur15_merge`.

```{r create_merged_data}
sur15_merge <- inner_join(sur15_a, sur15_b, by = "S.id")
```

The `sur15_merge` data includes many variables we don't need, so we'll prune down to the variables we'll need in what follows...

```{r variables_we_will_use}
sur15_m <- select(sur15_merge, S.id, r.pre, r.now, sex.x, height, weight, 
                  comfort.431, grades, load.431, prior.r, english,
                  medium, fiction, seat)
sur15_m
```

# The Survey Questions Studied Here

The 13 survey questions used in this demonstration project include the following. The names specified are those contained in the original `survey2015raw_a.csv` and `survey2015raw_b.csv` data files. 

## Rating Questions

For each of these, subjects gave a response between 0 and 100 indicating their agreement with the statement as presented. The scale was 0 = Strongly disagree, 100 = Strongly agree.

1. `r.pre`: Prior to taking 431, I was totally confident and comfortable with using R. (0 = Strongly Disagree, 100 = Strongly Agree)
2. `r.now`: Right now, I am totally confident and comfortable with using R. (0 = Strongly Disagree, 100 = Strongly Agree)
3. `comfort.431`: I am very comfortable with my understanding of the material discussed so far in 431.
4. `load.431`: So far, 431 has required me to do more work than a course has ever required of me.

## Other Quantitative Responses

5. `height`: What is your height, in inches?
6. `weight`: What is your weight, in pounds?

Here's a quick summary of these first six variables in their original, raw form:

```{r}
sur15_m %>% skim(r.pre, r.now, comfort.431, load.431, 
                 height, weight)
```

## Multi-Categorical Responses

7. `grades`: In your graduate and undergraduate educational experience, which of the following types of assignments have you received the HIGHEST grades for?
    - Available responses were "A. Individual Assignments", "B. Partner Assignments (you and 1 other student)", and "C. Group Assignments (you and 2 or more others)".
8. `medium`: Which medium do you use most to get your fictional stories (containing plot)?
    - Available Responses: "A. Movies", "B. Television", "C. Print (including books, comics, etc.)", and "D. Other".
9. `fiction`: Which type of fictional stories do you consume most?
    - Available Responses: "A. Comedy", "B. Drama", "C. Action", "D. Horror / Thriller", and "E. Fantasy / Science Fiction".
10. `seat`: In 431, do you USUALLY sit on the left side, in the middle or on the right side?
    - Available Responses: "A. On the left side as you face the screen", "B. In the middle of the room", and "C. On the right side (closest to the podium)".
    - Note that we used a different classroom then than we do now, so that these responses made more sense.

Here's a tabulation of those four variables in their raw form:

```{r}
sur15_m %>% select(grades, medium, fiction, seat) %>% summary()
```

**Note**: There is a missing `grades` value for one subject and we'll have to deal with that later.

## Binary Responses

11. `sex.x`: What is your sex? (Female, Male were the only two options presented)
12. `prior.r`: Before taking 431, had you ever used R before? (Yes, No)
13. `english`: Is English the language you speak better than any other? (Yes, No)

Here's a cross-tabulation of those three variables:

```{r}
sur15_m %>% count(sex.x, prior.r, english)
```

# Data Management: Tidying the Data

We're actually going to build a 15-variable data set, which we'll call `sur15` for this demonstration. The data set will need to contain each of the variables listed above. For each variable, we'll also check to see that all of the values fall in a reasonable range (with no results that fall outside of the parameters of how we are measuring the results) and we'll identify whether there are any missing values.

## Range and Missingness Checks: The seven "easy" variables

Seven variables come straight from the merged raw data in `sur15_merge`, with no further need for manipulation. All we need to do for these variables is a simple check of the range of values covered and whether or not there are any missing values. Generally, the easiest variables to deal with in cleaning have either quantitative responses or binary (No/Yes) responses, and no missing values. These are the seven variables we need that fall in that group.

1. `S.id` should have 53 unique values, specifically between 501 and 553.
2. `r.pre` should fall within the range 0-100.
3. `r.now` should also fall within the range 0-100.
4. `comfort.431` should be in 0-100.
5. `load.431` should be in 0-100.
6. `prior.r` should be No or Yes.
7. `english` should be No or Yes.

We'll use `Hmisc::describe()` here to do this work since it provides some additional details on the most extreme values in the data, but we could also have used `skim` if all we needed to do was look at the missing, minimum and maximum values.

```{r range_checks_for_seven_easy_variables}
sur15_m %>%
    select(S.id, r.pre, r.now, comfort.431, load.431, 
           prior.r, english) %>%
    Hmisc::describe()
```

## Reordering the No/Yes factors as Yes/No instead

You'll note that the `prior.r` and `english` variables have No listed before Yes in this output. We might want to change that, using the `fct_relevel` function from the `forcats` package in the `tidyverse`. If you feed this function a factor, and a list of levels, it will store those levels first, in order, and then retain any prior ordering. So, if we want "Yes" to show up first, we'll do...

```{r change_order_no-yes_variables}
sur15_m <- sur15_m %>%
    mutate(prior.r = fct_relevel(prior.r, "Yes"),
           english = fct_relevel(english, "Yes"))

sur15_m %>% select(prior.r, english) %>% summary()
```

OK. No missingness, and no values out of the range of our expectations. Good.

## Categorical Variables in need of Management

Five of our remaining variables are categorical (several have more than 2 categories.) In addition to checking for missingness and inappropriate values, we want to collapse some categories, or adjust names or labeling to mirror our desired codebook. We noted above that we have a missing `grades` value here, and we'll eventually have to decide what we want to do about that. But, for the moment, I'll address other concerns.

### Renaming `sex.x` as `sex`

When we merged the data, since `sex` was in both of the original files, the variable appeared in the merged file as both `sex.x` and `sex.y`. We selected `sex.x` only to keep (since the two were identical), but really, we want to recreate the variable as simply `sex`.

```{r create_sex_variable}
sur15_m <- sur15_m %>%
    mutate(sex = sex.x)
table(sur15_m$sex)
```

### Relabeling the levels of `grades` 

For `grades`, we want to wind up with a factor that has shorter level names, specifically: Individual, Partner and Group, in that order. We'll store the old version in `grades.old` so we can later do a little sanity check on our work, and then recode the `grades` information using the `fct_recode` function from the `forcats` package:

```{r recode_grades_information}
sur15_m <- sur15_m %>% 
    mutate(grades_old = grades,
           grades = fct_recode(grades_old, 
        "Individual" = "A. Individual Assignments",
        "Partner" = "B. Partner Assignments (you and 1 other student)",
        "Group" = "C. Group Assignments (you and 2 or more others)"))

# sanity check to ensure we recoded correctly
sur15_m %>% count(grades_old, grades) %>% knitr::kable()
```

We still have a missing value in `grades`, but we'll handle that later.

### Relabeling the levels of `seat` 

For `seat`, we want to wind up with a factor that has shorter level names, specifically: Left, Middle and Right, in that order. We'll store the old version in `seat.old` so we can later do a little sanity check on our work, and then recode the `seat` information using the `fct_recode` function from the `forcats` package:

```{r recode_seat_information}
sur15_m <- sur15_m %>%
    mutate(seat_old = seat,
           seat = fct_recode(seat_old, 
        "Left" = "A. On the left side as you face the screen",
        "Middle" = "B. In the middle of the room",
        "Right" = "C. On the right side (closest to the podium)"))
    
sur15_m %>% count(seat_old, seat) %>% knitr::kable()
```

### Collapsing and recoding levels of `medium`

For the `medium` variable, we want to collapse the Print and Other levels to form a three category variable (with levels Movies, TV and Other) called `medium_3`.

```{r collapsing_medium_factor}
sur15_m <- sur15_m %>%
    mutate(medium_3 = fct_recode(medium, 
                                 "Movies" = "A. Movies",
                                 "TV" = "B. Television",
                                 "Other" = "C. Print (including books, comics, etc.)",
                                 "Other" = "D. Other"))

sur15_m %>% count(medium, medium_3) # sanity check
```

### Collapsing and recoding levels of `fiction`

For the `fiction` variable, we want to form a four category variable (with levels Comedy, Drama, Fantasy/SciFi, Other) called `fiction_4`.

```{r collapsing_fiction_factor}
sur15_m <- sur15_m %>%
    mutate(fiction_4 = fct_recode(fiction, 
                                 "Comedy" = "A. Comedy",
                               "Drama" = "B. Drama",
                               "Fantasy/SciFi" = "E. Fantasy / Science Fiction",
                               "Other" = "C. Action",
                               "Other" = "D. Horror / Thriller"))

sur15_m %>% count(fiction, fiction_4) # sanity check
```

Actually, I'd like to reorder `fiction_4` to put Other last.

```{r reorder_fiction4c_Other_last}
sur15_m <- sur15_m %>%
    mutate(fiction_4 = fct_relevel(fiction_4, 
                                   "Comedy", "Drama",
                                   "Fantasy/SciFi", "Other"))
```


OK. Let's see what we have now...

```{r revised_values_medium_fiction}
sur15_m %>%
    select(medium_3, fiction_4) %>%
    table() %>% 
    knitr::kable()
```


## Combining `height` and `weight` into `bmi` and Specifying `NA` for Implausible Values

The last three variables we need are `height` and `weight`, and calculated `bmi`. Following the approach used in the Data Management materials posted as part of the Project Instructions after the Proposal, we will calculate `bmi` (body-mass index) from the available `height` (inches) and `weight` (pounds) data. The BMI formula for inches and pounds is available at http://www.bmi-calculator.net/bmi-formula.php. A reasonable range for BMI values is probably about 15 to 50.

```{r creating_bmi}
sur15_m <- sur15_m %>%
    mutate(bmi = 703 * weight / height^2)

Hmisc::describe(~ bmi, data = sur15_m)
```

Those two smallest calculated `bmi` values seem impossibly low, and the highest `bmi` seems impossibly high. Let's look at the heights and weights involved. A reasonable guess is that no one in the class was less than 4 feet tall (48 inches) nor were they greater than 7 feet tall (84 inches), and that no one was outside 80 - 400 pounds.

```{r describing_height_weight}
sur15_m %>%
  select(height, weight) %>%
  Hmisc::describe()
```

The subjects with heights of 22.83 inches and 217 inches seem implausible, and the subject with weight 1 pound is also not reasonable. If we change those values to missing, we'll better describe the believable results. I'll instruct R to change heights less than 48 inches and greater than 84 inches to `NA`, and also to change the weights less than 80 pounds to `NA` and those greater than 400 pounds (which we didn't see here) to `NA`.

```{r recode_implausible_height_weight_as_NA}
sur15_m <- sur15_m %>%
    mutate(height_old = height, weight_old = weight) %>%
    mutate(height = replace(height, height < 48, NA),
           height = replace(height, height > 84, NA),
           weight = replace(weight, weight < 80, NA),
           weight = replace(weight, weight > 400, NA)) %>%
    mutate(bmi = 703 * weight / height ^2)

sur15_m %>% skim(height, weight, bmi)
```

So now, we have 2 missing heights, 1 missing weight, and we have calculated BMI results, with 3 missing values. 

## Cleaning Up to get to our final tibble

Let's build a tibble called `sur15` that contains only the fifteen variables in our code book.

```{r create_sur15}
sur15 <- sur15_m %>%
    select(S.id, r.pre, r.now, sex, height, weight, bmi, 
           comfort.431, grades, load.431, prior.r, english, 
           medium_3, fiction_4, seat)

sur15
```

### Identifying and Dealing with Missing Values

We can count the number of missing observations in each variable, with ...

```{r na_pattern_in_sur15}
sur15 %>% summarize_all(funs(sum(is.na(.)))) %>%
    knitr::kable()
```

And see the subjects who have missing values with

```{r}
sur15 %>% filter(!complete.cases(.)) %>%
    knitr::kable()
```

In our sample of respondents, we have:

- 49 subjects with no missing values, 
- 1 subject (`S.id` = 516) who is missing `grades`, 
- 2 subjects (`S.id` = 504 and 529) who are missing `height` and `bmi`, and 
- 1 subject (`S.id` = 550) who is missing `weight` and `bmi`. 

So we'll have to keep that missingness in mind when we do work with `bmi` or `grades` in the analyses that follow.

```{r cleanup, echo = FALSE}
rm(sur15_a, sur15_b, sur15_m, sur15_merge)
```

# The Final, Clean Codebook

The 15 variables in our tidy data set for this demonstration are as follows. The Type column indicates the number of levels in each categorical (factor) variable. Recall that we have missing data in `height`, `weight`, `bmi` and `grades`.

Variable      | Type  | Description / Levels
--------- | :---: | --------------------------------------------
`S.id`        | ID    | subject code (501-533)
`r.pre`       | Quant | 0 (SD) - 100 (SA) with Prior to taking EPBI 431, I was totally confident and comfortable with using R.
`r.now`       | Quant | 0 (SD) - 100 (SA) with Right now, I am totally confident and comfortable with using R.
`sex`         | Cat-2 | female, male
`height`      | Quant | What is your height, in inches [**2 NA**]
`weight`      | Quant | What is your weight, in pounds [**1 NA**]
`bmi`         | Quant | 703 x `weight`/(`height` squared) [**3 NA**]
`comfort.431` | Quant | 0 (SD) - 100 (SA) with I am very comfortable with my understanding of the material discussed so far in EPBI 431.
`grades`      | Cat-3 | Individual, Partner, Group: In your graduate and undergraduate educational experience, which of the following types of assignments have you received the HIGHEST grades for? [**1 NA**]
`load.431`    | Quant | 0 (SD) - 100 (SA) with So far, EPBI 431 has required me to do more work than a course has ever required of me.
`prior.r`     | Cat-2 | yes, no: Before taking 431, had you ever used R before?
`english`     | Cat-2 | yes, no: Is English the language you speak better than any other?
`medium_3`   | Cat-3 | Movies, TV, Other: Which medium do you use most to get your fictional stories (containing plot)?
`fiction_4`  | Cat-4 | Comedy, Drama, Fantasy/SciFi, Other: Which type of fictional stories do you consume most?
`seat`        | Cat-3 | Left, Middle, Right: In EPBI 431, do you USUALLY sit on the left side, in the middle or on the right side (closest to the podium)?

# Analysis 1a: Compare 2 Population Means using Paired Samples

We'll compare the `r.now` scores to `r.pre` scores. The scores are paired by subject, as each subject gives us both a `r.pre` and `r.now` score, and computing and assessing within-subject differences in comfort with R makes sense, because we are interested in the change in each person's comfort level. We'll generally use `r.now - r.pre` in our calculations, so that positive numbers indicate improvements in confidence. **Note that we'll use a 90% confidence level throughout this demonstration project for all analyses, and I encourage you to do this in your actual Project Study 1 work, as well.**

## Compute and summarize the paired differences

The natural first step is to compute paired differences between the `r.now` and `r.pre` samples, and then use graphical and numerical summaries to assess whether the sample (of differences) can be assumed to follow a Normal distribution. First, we'll calculate the paired differences.

```{r compute_paired_differences}
sur15 <- sur15 %>%
    mutate(r_diff = r.now - r.pre)

Hmisc::describe(~ r_diff, data = sur15)
```

OK. It appears that we have successfully subtracted the PRE data from the NOW data, and everyone has a difference of at least zero. But we have a lot of people (8) who have a value of 0. Now, we'll assess whether or not a Normal distribution might be a reasonable model for the data.

### Graphical Summaries to Assess Normality

We should start by looking at the distribution of these 53 values of `r_diff`.  As we've seen, there's a floor effect at zero.

We could use the `fd_bins` function from `Love-boost.R` to determine the number of bins in a histogram...

```{r}
fd_bins(sur15$r_diff)
```

A histogram with 4 bins won't give us a lot of information. Perhaps we should focus instead on a Normal Q-Q plot and boxplot with violin? We'll draw all three here.

```{r}
p1 <- ggplot(sur15, aes(x = r_diff)) +
    geom_histogram(fill = "slateblue", col = "white", 
                   bins = 4) + 
    labs(x = "R Comfort Rating Difference") +
    theme_bw()

p2 <- ggplot(sur15, aes(sample = r_diff)) +
    geom_qq(col = "slateblue") + geom_qq_line(col = "red") + 
    labs(y = "Observed Difference in R Comfort Rating") +
    theme_bw()

p3 <- ggplot(sur15, aes(x = "n = 53", y = r_diff)) +
    geom_violin() + 
    geom_boxplot(fill = "slateblue", width = 0.3, notch = TRUE) + 
    labs(y = "Current - PreClass Difference in R Comfort Rating",
         x = "") +
    theme_bw()

gridExtra::grid.arrange(p1, p2, p3, nrow = 1, top = "Most Students Improved R Comfort Ratings during 431")
```

### Numerical Summaries to Assess Normality

In addition to running the usual summary statistics, we could also calculate skew~1~, to help assess the potential for serious asymmetry, and we could assess whether the Empirical Rule holds well for these differences, using the `skew1` and `Emp_Rule` functions within the `Love-boost.R` script.

```{r numerical_summaries_for_paired_differences}
mosaic::favstats(~ r_diff, data = sur15)
skew1(sur15$r_diff)
Emp_Rule(sur15$r_diff)
```

With just 53 observations, it will be a little difficult to get a clear picture of whether a Normal approximation is reasonable or not. I would conclude that a bootstrap approach would be a better choice here than a Normal model for the paired differences, owing to the floor effect (many zeros) in the paired differences. The data are a bit skewed, although they don't quite sneak over the 0.2 cutoff for skew~1~, and the Empirical Rule is a bit off expectations if the differences truly were sampled from a Normal distribution.

## Did Pairing Help Reduce Nuisance Variation?

We would expect a strong correlation between the `r.pre` and `r.now` scores in this repeated measures analysis where each subject is assessing both their confidence before the class and then again during the class. To assess whether pairing helped reduce nuisance variation, I'll build a scatterplot of the `r.pre` and `r.now` scores, supplemented by a Pearson correlation coefficient. Since we have so many ties in the data, with two or more points in the same place, I'll use `geom_jitter` rather than `geom_point` to plot the points. The larger the correlation, the more that pairing will help reduce the impact of differences between subjects on the `r.pre` score on the comparison we're trying to make. 

```{r scatterplot_for_paired_diffs}
ggplot(sur15, aes(x = r.pre, y = r.now)) +
    geom_jitter(col = "slateblue") +
    geom_smooth(method = "lm", col = "red") +
    theme_bw() +
    labs(title = "Jittered Scatterplot shows moderately strong relationship",
         subtitle = "especially for those starting above 0")
```

For people with a `r.pre` score greater than zero, we see a pretty strong linear relationship between `r.pre` and `r.now`.

```{r correlation_paired_diffs}
sur15 %>% select(r.pre, r.now) %>% cor(.) %>% 
    round(digits = 3) %>% knitr::kable()
```

The Pearson correlation is quite strong at `r round(cor(sur15$r.pre, sur15$r.now), 3)` so that a linear model using the `r.pre` score accounts for a reasonably large fraction (`r round(100*(cor(sur15$r.pre, sur15$r.now)^2),1)`%) of the variation in `r.now` scores.

- If the Pearson correlation had been small (perhaps less than 0.2), we might conclude that pairing wouldn't be exceptionally helpful, but if the samples are meant to be paired, we should still do a paired samples analysis, but such a small correlation would imply that an independent samples comparison would come to about the same conclusion.

## Building Confidence Intervals

As you'll recall, we have three primary methods for building confidence intervals in a paired samples analysis:

- The Paired t test
- The Wilcoxon Signed Rank test
- The Bootstrap, using `smean.cl.boot`

Let's run each of the three here just so you have the code, even though, as mentioned, I'd be most interested in what the bootstrap approach suggests, owing to the modest non-Normality we see in the sample of differences. In each case, we'll build a 90% confidence interval for the population mean (or pseudo-median, in the case of the Signed Rank test) of the `r.now - r.pre` differences.

### The Paired t test approach

Here is a 90% confidence interval for the population mean of the paired `r.now - r.pre` differences.

```{r paired_t_test}
t.test(sur15$r_diff, conf.level = .90)
```

- The point estimate for the population mean of the differences is 35.45, indicating that the average subject rated agreement with the statement about confidence in R 35 points higher now than when they started the class.
- Our 90% confidence interval for the population mean of the differences is (28.9, 42.1)
- Here, I've assumed a two-sided confidence interval and testing procedure\footnote{In this case, a one-sided test might also have been a good choice, since we don't anticipate people will actually admit to being less confident about R after taking the course.}. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 3.5 x 10^-12^) that there is a statistically significant difference between the `r.pre` and `r.now` scores.
- The assumptions of the paired t test are 
    + that the matched differences are independent of each other, 
    + that the matched differences represent a random sample of the population of possible matched differences, 
    + and that the matched differences are drawn from a Normally distributed population. 
    + The last of these assumptions is hard to justify given these data.

### The Wilcoxon signed rank test approach

Here is a 90% confidence interval for the population pseudo-median of the paired `r.now - r.pre` differences, as estimated by the Wilcoxon signed rank approach.

```{r signed_rank_test_and_CI}
wilcox.test(sur15$r_diff, conf.level = .90, conf.int = TRUE, exact = FALSE)
```

- The point estimate for the population pseudo-median of the differences is 40, indicating that the average subject rated agreement with the statement about confidence in R 40 points higher now than when they started the class. Note that this is meaningfully different from the sample median difference, which was 30, and that's because there was some skew in the sample data. The interpretation of the Wilcoxon approach is easiest for data that are light-tailed or heavy-tailed, but still generally symmetric.
- Our 90% confidence interval for the population pseudo-median of the differences is (35, 47.5)
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 5.1 x 10^-9^) that there is a statistically significant difference between `r.pre` and `r.now` scores.
- The assumptions of the Wilcoxon signed rank procedure are 
    + that the matched differences are independent of each other, 
    + that the matched differences represent a random sample of the population of possible matched differences, 
    + and that the matched differences are drawn from a population that is symmetric, but potentially light-tailed, or even outlier-prone 
    + The last of these assumptions is hard to justify given these data.
    
### The Bootstrap approach for the mean from paired samples

Here is a 90% confidence interval for the population mean of the paired `r.now - r.pre` differences, as estimated by a bootstrap approach using a random seed of `431`. (*Note*: when you set a seed for this or other analyses in the project, pick something other than `431`.)

```{r bootstrap_for_paired_samples}
set.seed(431)
Hmisc::smean.cl.boot(sur15$r_diff, conf.int = 0.90)
```

- The point estimate for the population mean of the differences is 35.45, indicating that the average subject rated agreement with the statement about confidence in R 35 points higher now than when they started the class.
- Our 90% confidence interval for the population mean of the differences is (29.1, 42.0), which is fairly close to what we got from the paired t test, as it turns out.
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, at the 10% significance level, that there is a statistically significant difference between the `r.pre` and `r.now` scores.
- The assumptions of this bootstrap procedure are 
    + that the matched differences are independent of each other, and
    + that the matched differences represent a random sample of the population of possible matched differences, 

My conclusion would be to use the bootstrap in this case, as it is most justified by my assessment of Normality, and my point estimate and 90% confidence interval for the improvement in R comfort levels is 35.45 (90% CI: 29.1, 42.0) points on the 0-100 scale.

# Analysis 1b: Compare 2 Population Means using Independent Samples

We'll compare `bmi` by `sex` in this analysis using independent samples. We're comparing the mean `bmi` of the population represented by the male respondents to the mean `bmi` of the population represented by the female respondents. There is nothing to suggest that the two samples (male `bmi` and female `bmi` values) are paired or matched in any way. There is no link between, for example, the first male subject's `bmi` and any particular female subject's `bmi`. Plus, as we'll see, there are different numbers of male and female subjects, so there's no way their `bmi` values could be paired. As a result, we're going to be interested in looking at the two samples separately (males and females) to help us understand issues related to hypothesis testing assumptions. **Note that we'll use a 90% confidence level throughout this demonstration project for all analyses, and I encourage you to do this in your actual Project Study 1 work, as well.**

## Summarizing the Distributions for each of the two samples

I'll start by looking at the range of the `bmi` data within each sex.

```{r bmi_by_sex}
mosaic::favstats(bmi ~ sex, data = sur15)
```

As we have previously seen, we have three missing BMI values. We could either impute these values, or remove those cases for this analysis. In this case, I'll remove the three missing values, and create a new data set called `sur15_1b` that contains only the variables I will use in this Analysis, and only the cases where `bmi` is available.

### A New Data Set including only those with `bmi` data

```{r drop_missing_bmi}
sur15_1b <- sur15 %>%
  filter(complete.cases(bmi)) %>%
  select(S.id, sex, bmi)

mosaic::favstats(bmi ~ sex, data = sur15_1b) %>% 
  knitr::kable()
```

Next, we'll use graphical and numerical summaries to assess whether the samples (of males, and of females, separately) can *each* be modeled appropriately by a Normal distribution. 

### Graphical Summaries

Let's build a comparison boxplot (with notches and violins) to start.

```{r boxplot_for_1b}
ggplot(sur15_1b, aes(x = sex, y = bmi)) + 
  geom_violin(fill = "white") +
  geom_boxplot(aes(fill = sex), width = 0.3, notch = TRUE) +
  guides(fill = FALSE) +
  labs(title = "BMI data somewhat right skewed for Males and Females",
       subtitle = "n = 50 Students in 431: Fall 2015",
       x = "", y = "Body Mass Index") +
  theme_bw()
```

I see a few candidate outliers in the female data on the high end, which suggest some potential for meaningful skew, and one high candidate outlier and some sign of right skew also among the male subjects. 

We could also build a pair of Normal Q-Q plots.

```{r qqplots_for_1b}
ggplot(sur15_1b, aes(sample = bmi, col = sex)) +
  geom_qq() + geom_qq_line() +
  facet_wrap(~ sex) +
  guides(col = FALSE) +
  theme_bw() +
  labs(y = "Observed BMI values",
       title = "Neither Male Nor Female BMI are fit well by a Normal model")
```

There's room for concern about whether a test that requires Normal distributions in the populations is a good choice here. With these small sample sizes, we'd probably be better off not making too many strong assumptions.

### Numerical Summaries

We have 24 female and 26 male subjects with known BMI values.

```{r numerical_summaries_1b}
mosaic::favstats(bmi ~ sex, data = sur15_1b) %>% 
  knitr::kable()
```

The skew~1~ values can be calculated from these summary statistics, as follows...

```{r calculating_skew1_for1b}
sur15_1b %>% group_by(sex) %>%
  summarize(skew1 = round((mean(bmi) - median(bmi))/sd(bmi), 3))
```

Or we can ask for them  with the `skew1` function from the `Love-boost.R` script...

```{r using_skew1_function_for_1b}
by(sur15_1b$bmi, sur15_1b$sex, skew1)
```

It looks like the right skew is large enough in each group to warrant avoiding tests that require Normality. So again it looks like it's not reasonable to assume Normality here.

## Building Confidence Intervals

As you'll recall, we have four available methods for building confidence intervals in an independent samples analysis:

- Welch's t test (t test without assuming equal variances)
- The Pooled t test (t test with equal variances assumed)
- The Wilcoxon-Mann-Whitney Rank Sum Test
- The Bootstrap, using `bootdif`

Let's run each of the four here just so you have the code, even though, as mentioned, I'd be most interested in what the bootstrap approach or the rank sum test suggests, owing to the fact that the samples aren't well described by Normal models. In each case, we'll build a 90% confidence interval for the population mean (or another measure of central tendency, in the case of the Rank Sum test) comparing `bmi` for females and males.

### The Welch's t test approach

With a nearly balanced design (24 females and 26 males), it is unlikely that the assumption of equal population variances will make much of a difference here, so we might expect the Welch t test and pooled t test to look similar. Neither is a great choice here, due to the samples showing some non-Normality. Regardless, here is a 90% confidence interval for the difference between the female and male population mean `bmi` based on Welch's test.

```{r Welch_t_test}
t.test(bmi ~ sex, data = sur15_1b, conf.level = 0.90)
```

- The point estimates for the two population `bmi` means are 22.9 for females and 24.9 for males, so the average male has a BMI estimated to be about 2.0 points higher than the average for females, based on our samples. 
- Our 90% confidence interval for the difference (Male - Female) of the population means is (0.1, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not quite contain zero) or the *p* value (which is 0.084) that there is a statistically significant difference between the true means of the male and female `bmi` levels.
- The assumptions of the Welch's t test are 
    + that the samples in each group are drawn independently of each other, 
    + that the samples in each group represent a random sample of the population of interest, 
    + and that the samples in each group are drawn from a Normally distributed population. 
    + The last of these assumptions is hard to justify given these data.

### The Pooled t test (t test with equal variances)

The pooled t test, of course, actually adds an assumption (that either the sample sizes or the population variances are equal) to the assumptions of the Welch test. With a nearly balanced design (24 females and 26 males), it is unlikely that the assumption of equal population variances will make much of a difference here, so we might expect the Welch t test and pooled t test to look similar. Neither is a great choice here, due to the samples showing some non-Normality. Regardless, here is a 90% confidence interval for the difference between the female and male population mean `bmi` based on the pooled t test.

```{r pooled_t_test}
t.test(bmi ~ sex, data = sur15_1b, conf.level = .90, var.equal = TRUE)
```

- The point estimates for the two population `bmi` means are still 22.9 for females and 24.9 for males, so the average male has a BMI estimated to be about 2.0 points higher than the average for females, based on our samples. 
- Our 90% confidence interval for the difference (Male - Female) of the population means is again (0.1, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not quite contain zero) or the *p* value (which is 0.086) that there is a statistically significant difference between the true means of the male and female `bmi` levels.
- The assumptions of the pooled t test are 
    + that the samples in each group are drawn independently of each other, 
    + that the samples in each group represent a random sample of the population of interest, 
    + the samples in each group are drawn from a Normally distributed population, 
    + *and* that either the sample sizes or the population variances are equal.
    + The Normality assumption remains hard to justify given these data, so we should look at alternatives.

### The Wilcoxon-Mann-Whitney rank sum test

The first test we'll look that doesn't require Normality is the Wilcoxon-Mann-Whitney rank sum test. The main problem with this approach is that it doesn't estimate the difference in population means, but rather it estimates a location shift for the distribution as a whole. Here is a 90% confidence interval for the difference between the female and male population `bmi` distributions based on the rank sum approach.

```{r rank_sum_test}
wilcox.test(sur15_1b$bmi ~ sur15_1b$sex, conf.level = .90, conf.int = TRUE, exact = FALSE)
```

- The estimated location shift in population `bmi` across the two sexes is 2.15.
- Our 90% confidence interval for the location shift (Male - Female) of the populations is (0.6, 3.3).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 0.016) that there is a statistically significant difference between the true locations of the male and female `bmi` levels.
- The assumptions of the rank sum test are 
    + that the samples in each group are drawn independently of each other, 
    + *and* that the samples in each group represent a random sample of the population of interest, 

So the Wilcoxon test is a stronger result than either t test, because it doesn't have any serious assumption violations. It is also in this case a more statistically significant result because the confidence interval isn't so close to zero (and thus the *p* value is smaller.)

### The Bootstrap for comparing means from two independent samples

The other approach we have for independent samples comparisons that doesn't require Normality is the bootstrap, and specifically, the `bootdif` function. This approach returns to estimating the difference in population means, but gives a different answer depending on the choice of random number seed. Here is a 90% confidence interval for the difference between the female and male population `bmi` distributions based on the bootstrap using a seed of `431`. (*Note*: when you set a seed for this or other analyses in the project, pick something other than `431`.)

```{r bootdif_for_1b}
set.seed(431) 
bootdif(sur15_1b$bmi, sur15_1b$sex, conf.level = 0.90)
```

- The population mean BMI in Males is estimated to be about 2.0 points higher than the population mean BMI for Females, based on our samples. So the mean differences' point estimate is 2.0
- Our 90% confidence interval for the difference (Male - Female) of the population means is (0.2, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) that there is a statistically significant difference (at the 10% significance level, since we have a 90% confidence interval) between the true means of the male and female `bmi` levels.
- The assumptions of this bootstrap procedure are:
    + that the samples in each group are drawn independently of each other, 
    + *and* that the samples in each group represent a random sample of the population of interest, 

So, I think either the bootstrap or rank sum procedure would be appropriate here, due to the non-Normality in the samples. In either case, at the 10% significance level, there is a statistically significant difference between the population mean (or in the rank sum case, location of) BMI for males and the population mean (or location of) BMI for females, based on our sample of 50 respondents.

# Analyses 2-6

=======
---
title: "431 Project Study 1 Demonstration"
author: "Thomas E. Love"
date: "`r Sys.Date()`"
linkcolor: blue
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    code_folding: show
---

# Introduction 

This document demonstrates the sorts of analyses we are asking you to complete in the context of Task E of your project for Study 1 (using the class survey.) We will use data from a prior class survey, gathered in two data files (called `survey2015raw_a` and `survey2015raw_b`) available on [the Data and Code page of our website](https://github.com/THOMASELOVE/431-2018-data).

# R Preliminaries and Data Load/Merge

## Initial Setup and Package Loads in R 

```{r knitr_init, cache=FALSE, message = FALSE, warning = FALSE}
#library(pander); library(mice); library(Epi)
#library(gridExtra); library(vcd); library(Hmisc)
#library(mosaic); library(magrittr) 

library(knitr); library(rmdformats); library(skimr) 
library(tidyverse) 

source("Love-boost.R")

## Global options

options(max.print="75")
opts_chunk$set(comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)

## Skim options (leave out histograms)

skimr::skim_with(numeric = list(hist = NULL),
                 integer = list(hist = NULL))
```

## Loading the Raw Data into R 

```{r data_load, message = FALSE}
sur15_a <- read.csv("survey2015raw_a.csv") %>% tbl_df
sur15_b <- read.csv("survey2015raw_b.csv") %>% tbl_df
```

## Merging the Two Raw Data Sets 

The `survey2015raw_a.csv` file contains, among other things, the `sex`, `english`, `prior.r`, `height`, `weight`, `comfort.431`, and `load.431` variables. The other variables we'll use are in the `survey2015raw_a.csv` file.

We'll begin by merging the `sur15_a` and `sur15_b` tibbles (each of which contains the linking variable `S.id`, into a data frame called `sur15_merge`.

```{r create_merged_data}
sur15_merge <- inner_join(sur15_a, sur15_b, by = "S.id")
```

The `sur15_merge` data includes many variables we don't need, so we'll prune down to the variables we'll need in what follows...

```{r variables_we_will_use}
sur15_m <- select(sur15_merge, S.id, r.pre, r.now, sex.x, height, weight, 
                  comfort.431, grades, load.431, prior.r, english,
                  medium, fiction, seat)
sur15_m
```

# The Survey Questions Studied Here

The 13 survey questions used in this demonstration project include the following. The names specified are those contained in the original `survey2015raw_a.csv` and `survey2015raw_b.csv` data files. 

## Rating Questions

For each of these, subjects gave a response between 0 and 100 indicating their agreement with the statement as presented. The scale was 0 = Strongly disagree, 100 = Strongly agree.

1. `r.pre`: Prior to taking 431, I was totally confident and comfortable with using R. (0 = Strongly Disagree, 100 = Strongly Agree)
2. `r.now`: Right now, I am totally confident and comfortable with using R. (0 = Strongly Disagree, 100 = Strongly Agree)
3. `comfort.431`: I am very comfortable with my understanding of the material discussed so far in 431.
4. `load.431`: So far, 431 has required me to do more work than a course has ever required of me.

## Other Quantitative Responses

5. `height`: What is your height, in inches?
6. `weight`: What is your weight, in pounds?

Here's a quick summary of these first six variables in their original, raw form:

```{r}
sur15_m %>% skim(r.pre, r.now, comfort.431, load.431, 
                 height, weight)
```

## Multi-Categorical Responses

7. `grades`: In your graduate and undergraduate educational experience, which of the following types of assignments have you received the HIGHEST grades for?
    - Available responses were "A. Individual Assignments", "B. Partner Assignments (you and 1 other student)", and "C. Group Assignments (you and 2 or more others)".
8. `medium`: Which medium do you use most to get your fictional stories (containing plot)?
    - Available Responses: "A. Movies", "B. Television", "C. Print (including books, comics, etc.)", and "D. Other".
9. `fiction`: Which type of fictional stories do you consume most?
    - Available Responses: "A. Comedy", "B. Drama", "C. Action", "D. Horror / Thriller", and "E. Fantasy / Science Fiction".
10. `seat`: In 431, do you USUALLY sit on the left side, in the middle or on the right side?
    - Available Responses: "A. On the left side as you face the screen", "B. In the middle of the room", and "C. On the right side (closest to the podium)".
    - Note that we used a different classroom then than we do now, so that these responses made more sense.

Here's a tabulation of those four variables in their raw form:

```{r}
sur15_m %>% select(grades, medium, fiction, seat) %>% summary()
```

**Note**: There is a missing `grades` value for one subject and we'll have to deal with that later.

## Binary Responses

11. `sex.x`: What is your sex? (Female, Male were the only two options presented)
12. `prior.r`: Before taking 431, had you ever used R before? (Yes, No)
13. `english`: Is English the language you speak better than any other? (Yes, No)

Here's a cross-tabulation of those three variables:

```{r}
sur15_m %>% count(sex.x, prior.r, english)
```

# Data Management: Tidying the Data

We're actually going to build a 15-variable data set, which we'll call `sur15` for this demonstration. The data set will need to contain each of the variables listed above. For each variable, we'll also check to see that all of the values fall in a reasonable range (with no results that fall outside of the parameters of how we are measuring the results) and we'll identify whether there are any missing values.

## Range and Missingness Checks: The seven "easy" variables

Seven variables come straight from the merged raw data in `sur15_merge`, with no further need for manipulation. All we need to do for these variables is a simple check of the range of values covered and whether or not there are any missing values. Generally, the easiest variables to deal with in cleaning have either quantitative responses or binary (No/Yes) responses, and no missing values. These are the seven variables we need that fall in that group.

1. `S.id` should have 53 unique values, specifically between 501 and 553.
2. `r.pre` should fall within the range 0-100.
3. `r.now` should also fall within the range 0-100.
4. `comfort.431` should be in 0-100.
5. `load.431` should be in 0-100.
6. `prior.r` should be No or Yes.
7. `english` should be No or Yes.

We'll use `Hmisc::describe()` here to do this work since it provides some additional details on the most extreme values in the data, but we could also have used `skim` if all we needed to do was look at the missing, minimum and maximum values.

```{r range_checks_for_seven_easy_variables}
sur15_m %>%
    select(S.id, r.pre, r.now, comfort.431, load.431, 
           prior.r, english) %>%
    Hmisc::describe()
```

## Reordering the No/Yes factors as Yes/No instead

You'll note that the `prior.r` and `english` variables have No listed before Yes in this output. We might want to change that, using the `fct_relevel` function from the `forcats` package in the `tidyverse`. If you feed this function a factor, and a list of levels, it will store those levels first, in order, and then retain any prior ordering. So, if we want "Yes" to show up first, we'll do...

```{r change_order_no-yes_variables}
sur15_m <- sur15_m %>%
    mutate(prior.r = fct_relevel(prior.r, "Yes"),
           english = fct_relevel(english, "Yes"))

sur15_m %>% select(prior.r, english) %>% summary()
```

OK. No missingness, and no values out of the range of our expectations. Good.

## Categorical Variables in need of Management

Five of our remaining variables are categorical (several have more than 2 categories.) In addition to checking for missingness and inappropriate values, we want to collapse some categories, or adjust names or labeling to mirror our desired codebook. We noted above that we have a missing `grades` value here, and we'll eventually have to decide what we want to do about that. But, for the moment, I'll address other concerns.

### Renaming `sex.x` as `sex`

When we merged the data, since `sex` was in both of the original files, the variable appeared in the merged file as both `sex.x` and `sex.y`. We selected `sex.x` only to keep (since the two were identical), but really, we want to recreate the variable as simply `sex`.

```{r create_sex_variable}
sur15_m <- sur15_m %>%
    mutate(sex = sex.x)
table(sur15_m$sex)
```

### Relabeling the levels of `grades` 

For `grades`, we want to wind up with a factor that has shorter level names, specifically: Individual, Partner and Group, in that order. We'll store the old version in `grades.old` so we can later do a little sanity check on our work, and then recode the `grades` information using the `fct_recode` function from the `forcats` package:

```{r recode_grades_information}
sur15_m <- sur15_m %>% 
    mutate(grades_old = grades,
           grades = fct_recode(grades_old, 
        "Individual" = "A. Individual Assignments",
        "Partner" = "B. Partner Assignments (you and 1 other student)",
        "Group" = "C. Group Assignments (you and 2 or more others)"))

# sanity check to ensure we recoded correctly
sur15_m %>% count(grades_old, grades) %>% knitr::kable()
```

We still have a missing value in `grades`, but we'll handle that later.

### Relabeling the levels of `seat` 

For `seat`, we want to wind up with a factor that has shorter level names, specifically: Left, Middle and Right, in that order. We'll store the old version in `seat.old` so we can later do a little sanity check on our work, and then recode the `seat` information using the `fct_recode` function from the `forcats` package:

```{r recode_seat_information}
sur15_m <- sur15_m %>%
    mutate(seat_old = seat,
           seat = fct_recode(seat_old, 
        "Left" = "A. On the left side as you face the screen",
        "Middle" = "B. In the middle of the room",
        "Right" = "C. On the right side (closest to the podium)"))
    
sur15_m %>% count(seat_old, seat) %>% knitr::kable()
```

### Collapsing and recoding levels of `medium`

For the `medium` variable, we want to collapse the Print and Other levels to form a three category variable (with levels Movies, TV and Other) called `medium_3`.

```{r collapsing_medium_factor}
sur15_m <- sur15_m %>%
    mutate(medium_3 = fct_recode(medium, 
                                 "Movies" = "A. Movies",
                                 "TV" = "B. Television",
                                 "Other" = "C. Print (including books, comics, etc.)",
                                 "Other" = "D. Other"))

sur15_m %>% count(medium, medium_3) # sanity check
```

### Collapsing and recoding levels of `fiction`

For the `fiction` variable, we want to form a four category variable (with levels Comedy, Drama, Fantasy/SciFi, Other) called `fiction_4`.

```{r collapsing_fiction_factor}
sur15_m <- sur15_m %>%
    mutate(fiction_4 = fct_recode(fiction, 
                                 "Comedy" = "A. Comedy",
                               "Drama" = "B. Drama",
                               "Fantasy/SciFi" = "E. Fantasy / Science Fiction",
                               "Other" = "C. Action",
                               "Other" = "D. Horror / Thriller"))

sur15_m %>% count(fiction, fiction_4) # sanity check
```

Actually, I'd like to reorder `fiction_4` to put Other last.

```{r reorder_fiction4c_Other_last}
sur15_m <- sur15_m %>%
    mutate(fiction_4 = fct_relevel(fiction_4, 
                                   "Comedy", "Drama",
                                   "Fantasy/SciFi", "Other"))
```


OK. Let's see what we have now...

```{r revised_values_medium_fiction}
sur15_m %>%
    select(medium_3, fiction_4) %>%
    table() %>% 
    knitr::kable()
```


## Combining `height` and `weight` into `bmi` and Specifying `NA` for Implausible Values

The last three variables we need are `height` and `weight`, and calculated `bmi`. Following the approach used in the Data Management materials posted as part of the Project Instructions after the Proposal, we will calculate `bmi` (body-mass index) from the available `height` (inches) and `weight` (pounds) data. The BMI formula for inches and pounds is available at http://www.bmi-calculator.net/bmi-formula.php. A reasonable range for BMI values is probably about 15 to 50.

```{r creating_bmi}
sur15_m <- sur15_m %>%
    mutate(bmi = 703 * weight / height^2)

Hmisc::describe(~ bmi, data = sur15_m)
```

Those two smallest calculated `bmi` values seem impossibly low, and the highest `bmi` seems impossibly high. Let's look at the heights and weights involved. A reasonable guess is that no one in the class was less than 4 feet tall (48 inches) nor were they greater than 7 feet tall (84 inches), and that no one was outside 80 - 400 pounds.

```{r describing_height_weight}
sur15_m %>%
  select(height, weight) %>%
  Hmisc::describe()
```

The subjects with heights of 22.83 inches and 217 inches seem implausible, and the subject with weight 1 pound is also not reasonable. If we change those values to missing, we'll better describe the believable results. I'll instruct R to change heights less than 48 inches and greater than 84 inches to `NA`, and also to change the weights less than 80 pounds to `NA` and those greater than 400 pounds (which we didn't see here) to `NA`.

```{r recode_implausible_height_weight_as_NA}
sur15_m <- sur15_m %>%
    mutate(height_old = height, weight_old = weight) %>%
    mutate(height = replace(height, height < 48, NA),
           height = replace(height, height > 84, NA),
           weight = replace(weight, weight < 80, NA),
           weight = replace(weight, weight > 400, NA)) %>%
    mutate(bmi = 703 * weight / height ^2)

sur15_m %>% skim(height, weight, bmi)
```

So now, we have 2 missing heights, 1 missing weight, and we have calculated BMI results, with 3 missing values. 

## Cleaning Up to get to our final tibble

Let's build a tibble called `sur15` that contains only the fifteen variables in our code book.

```{r create_sur15}
sur15 <- sur15_m %>%
    select(S.id, r.pre, r.now, sex, height, weight, bmi, 
           comfort.431, grades, load.431, prior.r, english, 
           medium_3, fiction_4, seat)

sur15
```

### Identifying and Dealing with Missing Values

We can count the number of missing observations in each variable, with ...

```{r na_pattern_in_sur15}
sur15 %>% summarize_all(funs(sum(is.na(.)))) %>%
    knitr::kable()
```

And see the subjects who have missing values with

```{r}
sur15 %>% filter(!complete.cases(.)) %>%
    knitr::kable()
```

In our sample of respondents, we have:

- 49 subjects with no missing values, 
- 1 subject (`S.id` = 516) who is missing `grades`, 
- 2 subjects (`S.id` = 504 and 529) who are missing `height` and `bmi`, and 
- 1 subject (`S.id` = 550) who is missing `weight` and `bmi`. 

So we'll have to keep that missingness in mind when we do work with `bmi` or `grades` in the analyses that follow.

```{r cleanup, echo = FALSE}
rm(sur15_a, sur15_b, sur15_m, sur15_merge)
```

# The Final, Clean Codebook

The 15 variables in our tidy data set for this demonstration are as follows. The Type column indicates the number of levels in each categorical (factor) variable. Recall that we have missing data in `height`, `weight`, `bmi` and `grades`.

Variable      | Type  | Description / Levels
--------- | :---: | --------------------------------------------
`S.id`        | ID    | subject code (501-533)
`r.pre`       | Quant | 0 (SD) - 100 (SA) with Prior to taking EPBI 431, I was totally confident and comfortable with using R.
`r.now`       | Quant | 0 (SD) - 100 (SA) with Right now, I am totally confident and comfortable with using R.
`sex`         | Cat-2 | female, male
`height`      | Quant | What is your height, in inches [**2 NA**]
`weight`      | Quant | What is your weight, in pounds [**1 NA**]
`bmi`         | Quant | 703 x `weight`/(`height` squared) [**3 NA**]
`comfort.431` | Quant | 0 (SD) - 100 (SA) with I am very comfortable with my understanding of the material discussed so far in EPBI 431.
`grades`      | Cat-3 | Individual, Partner, Group: In your graduate and undergraduate educational experience, which of the following types of assignments have you received the HIGHEST grades for? [**1 NA**]
`load.431`    | Quant | 0 (SD) - 100 (SA) with So far, EPBI 431 has required me to do more work than a course has ever required of me.
`prior.r`     | Cat-2 | yes, no: Before taking 431, had you ever used R before?
`english`     | Cat-2 | yes, no: Is English the language you speak better than any other?
`medium_3`   | Cat-3 | Movies, TV, Other: Which medium do you use most to get your fictional stories (containing plot)?
`fiction_4`  | Cat-4 | Comedy, Drama, Fantasy/SciFi, Other: Which type of fictional stories do you consume most?
`seat`        | Cat-3 | Left, Middle, Right: In EPBI 431, do you USUALLY sit on the left side, in the middle or on the right side (closest to the podium)?

# Analysis 1a: Compare 2 Population Means using Paired Samples

We'll compare the `r.now` scores to `r.pre` scores. The scores are paired by subject, as each subject gives us both a `r.pre` and `r.now` score, and computing and assessing within-subject differences in comfort with R makes sense, because we are interested in the change in each person's comfort level. We'll generally use `r.now - r.pre` in our calculations, so that positive numbers indicate improvements in confidence. **Note that we'll use a 90% confidence level throughout this demonstration project for all analyses, and I encourage you to do this in your actual Project Study 1 work, as well.**

## Compute and summarize the paired differences

The natural first step is to compute paired differences between the `r.now` and `r.pre` samples, and then use graphical and numerical summaries to assess whether the sample (of differences) can be assumed to follow a Normal distribution. First, we'll calculate the paired differences.

```{r compute_paired_differences}
sur15 <- sur15 %>%
    mutate(r_diff = r.now - r.pre)

Hmisc::describe(~ r_diff, data = sur15)
```

OK. It appears that we have successfully subtracted the PRE data from the NOW data, and everyone has a difference of at least zero. But we have a lot of people (8) who have a value of 0. Now, we'll assess whether or not a Normal distribution might be a reasonable model for the data.

### Graphical Summaries to Assess Normality

We should start by looking at the distribution of these 53 values of `r_diff`.  As we've seen, there's a floor effect at zero.

We could use the `fd_bins` function from `Love-boost.R` to determine the number of bins in a histogram...

```{r}
fd_bins(sur15$r_diff)
```

A histogram with 4 bins won't give us a lot of information. Perhaps we should focus instead on a Normal Q-Q plot and boxplot with violin? We'll draw all three here.

```{r}
p1 <- ggplot(sur15, aes(x = r_diff)) +
    geom_histogram(fill = "slateblue", col = "white", 
                   bins = 4) + 
    labs(x = "R Comfort Rating Difference") +
    theme_bw()

p2 <- ggplot(sur15, aes(sample = r_diff)) +
    geom_qq(col = "slateblue") + geom_qq_line(col = "red") + 
    labs(y = "Observed Difference in R Comfort Rating") +
    theme_bw()

p3 <- ggplot(sur15, aes(x = "n = 53", y = r_diff)) +
    geom_violin() + 
    geom_boxplot(fill = "slateblue", width = 0.3, notch = TRUE) + 
    labs(y = "Current - PreClass Difference in R Comfort Rating",
         x = "") +
    theme_bw()

gridExtra::grid.arrange(p1, p2, p3, nrow = 1, top = "Most Students Improved R Comfort Ratings during 431")
```

### Numerical Summaries to Assess Normality

In addition to running the usual summary statistics, we could also calculate skew~1~, to help assess the potential for serious asymmetry, and we could assess whether the Empirical Rule holds well for these differences, using the `skew1` and `Emp_Rule` functions within the `Love-boost.R` script.

```{r numerical_summaries_for_paired_differences}
mosaic::favstats(~ r_diff, data = sur15)
skew1(sur15$r_diff)
Emp_Rule(sur15$r_diff)
```

With just 53 observations, it will be a little difficult to get a clear picture of whether a Normal approximation is reasonable or not. I would conclude that a bootstrap approach would be a better choice here than a Normal model for the paired differences, owing to the floor effect (many zeros) in the paired differences. The data are a bit skewed, although they don't quite sneak over the 0.2 cutoff for skew~1~, and the Empirical Rule is a bit off expectations if the differences truly were sampled from a Normal distribution.

## Did Pairing Help Reduce Nuisance Variation?

We would expect a strong correlation between the `r.pre` and `r.now` scores in this repeated measures analysis where each subject is assessing both their confidence before the class and then again during the class. To assess whether pairing helped reduce nuisance variation, I'll build a scatterplot of the `r.pre` and `r.now` scores, supplemented by a Pearson correlation coefficient. Since we have so many ties in the data, with two or more points in the same place, I'll use `geom_jitter` rather than `geom_point` to plot the points. The larger the correlation, the more that pairing will help reduce the impact of differences between subjects on the `r.pre` score on the comparison we're trying to make. 

```{r scatterplot_for_paired_diffs}
ggplot(sur15, aes(x = r.pre, y = r.now)) +
    geom_jitter(col = "slateblue") +
    geom_smooth(method = "lm", col = "red") +
    theme_bw() +
    labs(title = "Jittered Scatterplot shows moderately strong relationship",
         subtitle = "especially for those starting above 0")
```

For people with a `r.pre` score greater than zero, we see a pretty strong linear relationship between `r.pre` and `r.now`.

```{r correlation_paired_diffs}
sur15 %>% select(r.pre, r.now) %>% cor(.) %>% 
    round(digits = 3) %>% knitr::kable()
```

The Pearson correlation is quite strong at `r round(cor(sur15$r.pre, sur15$r.now), 3)` so that a linear model using the `r.pre` score accounts for a reasonably large fraction (`r round(100*(cor(sur15$r.pre, sur15$r.now)^2),1)`%) of the variation in `r.now` scores.

- If the Pearson correlation had been small (perhaps less than 0.2), we might conclude that pairing wouldn't be exceptionally helpful, but if the samples are meant to be paired, we should still do a paired samples analysis, but such a small correlation would imply that an independent samples comparison would come to about the same conclusion.

## Building Confidence Intervals

As you'll recall, we have three primary methods for building confidence intervals in a paired samples analysis:

- The Paired t test
- The Wilcoxon Signed Rank test
- The Bootstrap, using `smean.cl.boot`

Let's run each of the three here just so you have the code, even though, as mentioned, I'd be most interested in what the bootstrap approach suggests, owing to the modest non-Normality we see in the sample of differences. In each case, we'll build a 90% confidence interval for the population mean (or pseudo-median, in the case of the Signed Rank test) of the `r.now - r.pre` differences.

### The Paired t test approach

Here is a 90% confidence interval for the population mean of the paired `r.now - r.pre` differences.

```{r paired_t_test}
t.test(sur15$r_diff, conf.level = .90)
```

- The point estimate for the population mean of the differences is 35.45, indicating that the average subject rated agreement with the statement about confidence in R 35 points higher now than when they started the class.
- Our 90% confidence interval for the population mean of the differences is (28.9, 42.1)
- Here, I've assumed a two-sided confidence interval and testing procedure\footnote{In this case, a one-sided test might also have been a good choice, since we don't anticipate people will actually admit to being less confident about R after taking the course.}. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 3.5 x 10^-12^) that there is a statistically significant difference between the `r.pre` and `r.now` scores.
- The assumptions of the paired t test are 
    + that the matched differences are independent of each other, 
    + that the matched differences represent a random sample of the population of possible matched differences, 
    + and that the matched differences are drawn from a Normally distributed population. 
    + The last of these assumptions is hard to justify given these data.

### The Wilcoxon signed rank test approach

Here is a 90% confidence interval for the population pseudo-median of the paired `r.now - r.pre` differences, as estimated by the Wilcoxon signed rank approach.

```{r signed_rank_test_and_CI}
wilcox.test(sur15$r_diff, conf.level = .90, conf.int = TRUE, exact = FALSE)
```

- The point estimate for the population pseudo-median of the differences is 40, indicating that the average subject rated agreement with the statement about confidence in R 40 points higher now than when they started the class. Note that this is meaningfully different from the sample median difference, which was 30, and that's because there was some skew in the sample data. The interpretation of the Wilcoxon approach is easiest for data that are light-tailed or heavy-tailed, but still generally symmetric.
- Our 90% confidence interval for the population pseudo-median of the differences is (35, 47.5)
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 5.1 x 10^-9^) that there is a statistically significant difference between `r.pre` and `r.now` scores.
- The assumptions of the Wilcoxon signed rank procedure are 
    + that the matched differences are independent of each other, 
    + that the matched differences represent a random sample of the population of possible matched differences, 
    + and that the matched differences are drawn from a population that is symmetric, but potentially light-tailed, or even outlier-prone 
    + The last of these assumptions is hard to justify given these data.
    
### The Bootstrap approach for the mean from paired samples

Here is a 90% confidence interval for the population mean of the paired `r.now - r.pre` differences, as estimated by a bootstrap approach using a random seed of `431`. (*Note*: when you set a seed for this or other analyses in the project, pick something other than `431`.)

```{r bootstrap_for_paired_samples}
set.seed(431)
Hmisc::smean.cl.boot(sur15$r_diff, conf.int = 0.90)
```

- The point estimate for the population mean of the differences is 35.45, indicating that the average subject rated agreement with the statement about confidence in R 35 points higher now than when they started the class.
- Our 90% confidence interval for the population mean of the differences is (29.1, 42.0), which is fairly close to what we got from the paired t test, as it turns out.
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, at the 10% significance level, that there is a statistically significant difference between the `r.pre` and `r.now` scores.
- The assumptions of this bootstrap procedure are 
    + that the matched differences are independent of each other, and
    + that the matched differences represent a random sample of the population of possible matched differences, 

My conclusion would be to use the bootstrap in this case, as it is most justified by my assessment of Normality, and my point estimate and 90% confidence interval for the improvement in R comfort levels is 35.45 (90% CI: 29.1, 42.0) points on the 0-100 scale.

# Analysis 1b: Compare 2 Population Means using Independent Samples

We'll compare `bmi` by `sex` in this analysis using independent samples. We're comparing the mean `bmi` of the population represented by the male respondents to the mean `bmi` of the population represented by the female respondents. There is nothing to suggest that the two samples (male `bmi` and female `bmi` values) are paired or matched in any way. There is no link between, for example, the first male subject's `bmi` and any particular female subject's `bmi`. Plus, as we'll see, there are different numbers of male and female subjects, so there's no way their `bmi` values could be paired. As a result, we're going to be interested in looking at the two samples separately (males and females) to help us understand issues related to hypothesis testing assumptions. **Note that we'll use a 90% confidence level throughout this demonstration project for all analyses, and I encourage you to do this in your actual Project Study 1 work, as well.**

## Summarizing the Distributions for each of the two samples

I'll start by looking at the range of the `bmi` data within each sex.

```{r bmi_by_sex}
mosaic::favstats(bmi ~ sex, data = sur15)
```

As we have previously seen, we have three missing BMI values. We could either impute these values, or remove those cases for this analysis. In this case, I'll remove the three missing values, and create a new data set called `sur15_1b` that contains only the variables I will use in this Analysis, and only the cases where `bmi` is available.

### A New Data Set including only those with `bmi` data

```{r drop_missing_bmi}
sur15_1b <- sur15 %>%
  filter(complete.cases(bmi)) %>%
  select(S.id, sex, bmi)

mosaic::favstats(bmi ~ sex, data = sur15_1b) %>% 
  knitr::kable()
```

Next, we'll use graphical and numerical summaries to assess whether the samples (of males, and of females, separately) can *each* be modeled appropriately by a Normal distribution. 

### Graphical Summaries

Let's build a comparison boxplot (with notches and violins) to start.

```{r boxplot_for_1b}
ggplot(sur15_1b, aes(x = sex, y = bmi)) + 
  geom_violin(fill = "white") +
  geom_boxplot(aes(fill = sex), width = 0.3, notch = TRUE) +
  guides(fill = FALSE) +
  labs(title = "BMI data somewhat right skewed for Males and Females",
       subtitle = "n = 50 Students in 431: Fall 2015",
       x = "", y = "Body Mass Index") +
  theme_bw()
```

I see a few candidate outliers in the female data on the high end, which suggest some potential for meaningful skew, and one high candidate outlier and some sign of right skew also among the male subjects. 

We could also build a pair of Normal Q-Q plots.

```{r qqplots_for_1b}
ggplot(sur15_1b, aes(sample = bmi, col = sex)) +
  geom_qq() + geom_qq_line() +
  facet_wrap(~ sex) +
  guides(col = FALSE) +
  theme_bw() +
  labs(y = "Observed BMI values",
       title = "Neither Male Nor Female BMI are fit well by a Normal model")
```

There's room for concern about whether a test that requires Normal distributions in the populations is a good choice here. With these small sample sizes, we'd probably be better off not making too many strong assumptions.

### Numerical Summaries

We have 24 female and 26 male subjects with known BMI values.

```{r numerical_summaries_1b}
mosaic::favstats(bmi ~ sex, data = sur15_1b) %>% 
  knitr::kable()
```

The skew~1~ values can be calculated from these summary statistics, as follows...

```{r calculating_skew1_for1b}
sur15_1b %>% group_by(sex) %>%
  summarize(skew1 = round((mean(bmi) - median(bmi))/sd(bmi), 3))
```

Or we can ask for them  with the `skew1` function from the `Love-boost.R` script...

```{r using_skew1_function_for_1b}
by(sur15_1b$bmi, sur15_1b$sex, skew1)
```

It looks like the right skew is large enough in each group to warrant avoiding tests that require Normality. So again it looks like it's not reasonable to assume Normality here.

## Building Confidence Intervals

As you'll recall, we have four available methods for building confidence intervals in an independent samples analysis:

- Welch's t test (t test without assuming equal variances)
- The Pooled t test (t test with equal variances assumed)
- The Wilcoxon-Mann-Whitney Rank Sum Test
- The Bootstrap, using `bootdif`

Let's run each of the four here just so you have the code, even though, as mentioned, I'd be most interested in what the bootstrap approach or the rank sum test suggests, owing to the fact that the samples aren't well described by Normal models. In each case, we'll build a 90% confidence interval for the population mean (or another measure of central tendency, in the case of the Rank Sum test) comparing `bmi` for females and males.

### The Welch's t test approach

With a nearly balanced design (24 females and 26 males), it is unlikely that the assumption of equal population variances will make much of a difference here, so we might expect the Welch t test and pooled t test to look similar. Neither is a great choice here, due to the samples showing some non-Normality. Regardless, here is a 90% confidence interval for the difference between the female and male population mean `bmi` based on Welch's test.

```{r Welch_t_test}
t.test(bmi ~ sex, data = sur15_1b, conf.level = 0.90)
```

- The point estimates for the two population `bmi` means are 22.9 for females and 24.9 for males, so the average male has a BMI estimated to be about 2.0 points higher than the average for females, based on our samples. 
- Our 90% confidence interval for the difference (Male - Female) of the population means is (0.1, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not quite contain zero) or the *p* value (which is 0.084) that there is a statistically significant difference between the true means of the male and female `bmi` levels.
- The assumptions of the Welch's t test are 
    + that the samples in each group are drawn independently of each other, 
    + that the samples in each group represent a random sample of the population of interest, 
    + and that the samples in each group are drawn from a Normally distributed population. 
    + The last of these assumptions is hard to justify given these data.

### The Pooled t test (t test with equal variances)

The pooled t test, of course, actually adds an assumption (that either the sample sizes or the population variances are equal) to the assumptions of the Welch test. With a nearly balanced design (24 females and 26 males), it is unlikely that the assumption of equal population variances will make much of a difference here, so we might expect the Welch t test and pooled t test to look similar. Neither is a great choice here, due to the samples showing some non-Normality. Regardless, here is a 90% confidence interval for the difference between the female and male population mean `bmi` based on the pooled t test.

```{r pooled_t_test}
t.test(bmi ~ sex, data = sur15_1b, conf.level = .90, var.equal = TRUE)
```

- The point estimates for the two population `bmi` means are still 22.9 for females and 24.9 for males, so the average male has a BMI estimated to be about 2.0 points higher than the average for females, based on our samples. 
- Our 90% confidence interval for the difference (Male - Female) of the population means is again (0.1, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not quite contain zero) or the *p* value (which is 0.086) that there is a statistically significant difference between the true means of the male and female `bmi` levels.
- The assumptions of the pooled t test are 
    + that the samples in each group are drawn independently of each other, 
    + that the samples in each group represent a random sample of the population of interest, 
    + the samples in each group are drawn from a Normally distributed population, 
    + *and* that either the sample sizes or the population variances are equal.
    + The Normality assumption remains hard to justify given these data, so we should look at alternatives.

### The Wilcoxon-Mann-Whitney rank sum test

The first test we'll look that doesn't require Normality is the Wilcoxon-Mann-Whitney rank sum test. The main problem with this approach is that it doesn't estimate the difference in population means, but rather it estimates a location shift for the distribution as a whole. Here is a 90% confidence interval for the difference between the female and male population `bmi` distributions based on the rank sum approach.

```{r rank_sum_test}
wilcox.test(sur15_1b$bmi ~ sur15_1b$sex, conf.level = .90, conf.int = TRUE, exact = FALSE)
```

- The estimated location shift in population `bmi` across the two sexes is 2.15.
- Our 90% confidence interval for the location shift (Male - Female) of the populations is (0.6, 3.3).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) or the *p* value (which is 0.016) that there is a statistically significant difference between the true locations of the male and female `bmi` levels.
- The assumptions of the rank sum test are 
    + that the samples in each group are drawn independently of each other, 
    + *and* that the samples in each group represent a random sample of the population of interest, 

So the Wilcoxon test is a stronger result than either t test, because it doesn't have any serious assumption violations. It is also in this case a more statistically significant result because the confidence interval isn't so close to zero (and thus the *p* value is smaller.)

### The Bootstrap for comparing means from two independent samples

The other approach we have for independent samples comparisons that doesn't require Normality is the bootstrap, and specifically, the `bootdif` function. This approach returns to estimating the difference in population means, but gives a different answer depending on the choice of random number seed. Here is a 90% confidence interval for the difference between the female and male population `bmi` distributions based on the bootstrap using a seed of `431`. (*Note*: when you set a seed for this or other analyses in the project, pick something other than `431`.)

```{r bootdif_for_1b}
set.seed(431) 
bootdif(sur15_1b$bmi, sur15_1b$sex, conf.level = 0.90)
```

- The population mean BMI in Males is estimated to be about 2.0 points higher than the population mean BMI for Females, based on our samples. So the mean differences' point estimate is 2.0
- Our 90% confidence interval for the difference (Male - Female) of the population means is (0.2, 3.9).
- Here, I've assumed a two-sided confidence interval and testing procedure. We conclude, either from the confidence interval (which does not contain zero) that there is a statistically significant difference (at the 10% significance level, since we have a 90% confidence interval) between the true means of the male and female `bmi` levels.
- The assumptions of this bootstrap procedure are:
    + that the samples in each group are drawn independently of each other, 
    + *and* that the samples in each group represent a random sample of the population of interest, 

So, I think either the bootstrap or rank sum procedure would be appropriate here, due to the non-Normality in the samples. In either case, at the 10% significance level, there is a statistically significant difference between the population mean (or in the rank sum case, location of) BMI for males and the population mean (or location of) BMI for females, based on our sample of 50 respondents.

# Analyses 2-6

>>>>>>> 55000bc8affd7e234e48b453cb7d76df35b46842
Coming soon.